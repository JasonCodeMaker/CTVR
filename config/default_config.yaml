# Default configuration for Continual Text-to-Video Retrieval

# Evaluation parameters
eval: false
eval_path: "outputs/debug"

# Data parameters
path_data: "data/MSRVTT_10_dataset.pkl"
dataset_name: "MSRVTT"
videos_dir: "datasets/msrvtt_data/MSRVTT_Videos" # "datasets/ActivityNet/Activity_Clip_Frames"
num_frames: 12
video_sample_type: "uniform"
input_res: 224

# Experiment parameters
project_name: "Debug"
api_key: "YOUR API KEY"
workspace: "YOUR WORKSPACE"
exp_name: "debug"
output_dir: "./outputs"
evals_per_epoch: 10
eval_window_size: 5
metric: "t2v"
grad_acc_steps: 4

training_type: "full_shot"
num_shots: 16
init_validation: true
store_vid_embed: false
load_best: true
benchmark: "anet_cap"
image_feature: false
frame_reduction: -1

# WISE-FT parameters
wise_mode: "None"
wise_strategy: "interpolation"
wise_alpha: 0.5

# Model parameters
huggingface: true
pre_trained: true
arch: "avg_pool"
clip_arch: "ViT-B/32"
embed_dim: 512

# Training parameters
loss: "clip"
clip_v_lr: 0.000001
clip_t_lr: 0.000001
noclip_lr: 0.00001
batch_size: 32
max_num_epochs: 3
weight_decay: 0.2
warmup_proportion: 0.1

# Frame pooling parameters
attention_temperature: 0.01
num_mha_heads: 1
transformer_dropout: 0.3

# System parameters
num_workers: 8
seed: 42
data_analysis: false

# MoE Adapter parameters
ffn_num: 64
ffn_adapt: false
ffn_option: "parallel"
ffn_adapt_where: "AdapterDoubleEncoder"
apply_moe: false
repeat_train: false
task_id: -1
multi_experts: false
num_experts: 22
is_train: false
frozen: false
autorouter: false
threshold: null
non_text: false
frozen_path: "frozen_list"